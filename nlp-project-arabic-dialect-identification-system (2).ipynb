{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Arabic Dialect Identification system\n\n**CCAI-413: Natural Language Processing project.**\n\n\n------------------------------------------------------------------\n\n# About \nIs an Arabic Dialect Identification system. its task of identifying the dialect of Arabic language in a text format. It is a challenging task due to the high variability of Arabic dialects and the lack of large-scale annotated datasets.  \n\n\n","metadata":{}},{"cell_type":"markdown","source":"# important Library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk \n\nnltk.download('wordnet')\nfrom nltk.stem.isri import ISRIStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2023-05-22T05:12:16.079611Z","iopub.execute_input":"2023-05-22T05:12:16.079960Z","iopub.status.idle":"2023-05-22T05:12:19.296723Z","shell.execute_reply.started":"2023-05-22T05:12:16.079929Z","shell.execute_reply":"2023-05-22T05:12:19.295634Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"markdown","source":"**Messages Dataset:**\n\nData contains tweets in different Arabic dialects.","metadata":{}},{"cell_type":"code","source":"# Load messages dataset\ntweets = pd.read_csv('/kaggle/input/aim-technologies-predict-the-dialectal-arabic/messages.csv',lineterminator='\\n')\ncolumn_names = ['id', 'tweets'] # list of column names\ntweets.columns = column_names # Label the columns\ntweets","metadata":{"execution":{"iopub.status.busy":"2023-05-22T05:12:19.299470Z","iopub.execute_input":"2023-05-22T05:12:19.301456Z","iopub.status.idle":"2023-05-22T05:12:22.399676Z","shell.execute_reply.started":"2023-05-22T05:12:19.301426Z","shell.execute_reply":"2023-05-22T05:12:22.398676Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                  id                                             tweets\n0       1.175358e+18   @Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .\n1       1.175416e+18  @7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...\n2       1.175450e+18                    @KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ\n3       1.175471e+18         @HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’\n4       1.175497e+18                 @hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº\n...              ...                                                ...\n458656  1.057419e+18  @mycousinvinnyys @hanyamikhail1 Ù…ØªÙ‡ÙŠØ§Ù„ÙŠ Ø¯ÙŠ Ø´ÙƒÙˆ...\n458657  1.055620e+18  @MahmoudWaked7 @maganenoo ÙÙŠ Ø·Ø±ÙŠÙ‚ Ù…Ø·Ø±ÙˆØ­ Ù…Ø±ÙƒØ² Ø¨...\n458658           NaN                                                  0\n458659  1.057419e+18  @mycousinvinnyys @hanyamikhail1 Ù…ØªÙ‡ÙŠØ§Ù„ÙŠ Ø¯ÙŠ Ø´ÙƒÙˆ...\n458660  1.055620e+18  @MahmoudWaked7 @maganenoo ÙÙŠ Ø·Ø±ÙŠÙ‚ Ù…Ø·Ø±ÙˆØ­ Ù…Ø±ÙƒØ² Ø¨...\n\n[458661 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.175358e+18</td>\n      <td>@Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.175416e+18</td>\n      <td>@7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.175450e+18</td>\n      <td>@KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.175471e+18</td>\n      <td>@HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.175497e+18</td>\n      <td>@hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>458656</th>\n      <td>1.057419e+18</td>\n      <td>@mycousinvinnyys @hanyamikhail1 Ù…ØªÙ‡ÙŠØ§Ù„ÙŠ Ø¯ÙŠ Ø´ÙƒÙˆ...</td>\n    </tr>\n    <tr>\n      <th>458657</th>\n      <td>1.055620e+18</td>\n      <td>@MahmoudWaked7 @maganenoo ÙÙŠ Ø·Ø±ÙŠÙ‚ Ù…Ø·Ø±ÙˆØ­ Ù…Ø±ÙƒØ² Ø¨...</td>\n    </tr>\n    <tr>\n      <th>458658</th>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>458659</th>\n      <td>1.057419e+18</td>\n      <td>@mycousinvinnyys @hanyamikhail1 Ù…ØªÙ‡ÙŠØ§Ù„ÙŠ Ø¯ÙŠ Ø´ÙƒÙˆ...</td>\n    </tr>\n    <tr>\n      <th>458660</th>\n      <td>1.055620e+18</td>\n      <td>@MahmoudWaked7 @maganenoo ÙÙŠ Ø·Ø±ÙŠÙ‚ Ù…Ø·Ø±ÙˆØ­ Ù…Ø±ÙƒØ² Ø¨...</td>\n    </tr>\n  </tbody>\n</table>\n<p>458661 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Dialect Dataset:**\n\nData contains the dialect (labels) of the tweets.","metadata":{}},{"cell_type":"code","source":"# Load dialect dataset\ndialects = pd.read_csv(\"/kaggle/input/aim-technologies-predict-the-dialectal-arabic/dialect_dataset.csv\")\ndialects","metadata":{"execution":{"iopub.status.busy":"2023-05-22T05:12:22.411718Z","iopub.execute_input":"2023-05-22T05:12:22.412332Z","iopub.status.idle":"2023-05-22T05:12:22.703647Z","shell.execute_reply.started":"2023-05-22T05:12:22.412297Z","shell.execute_reply":"2023-05-22T05:12:22.702614Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                         id dialect\n0       1175358310087892992      IQ\n1       1175416117793349632      IQ\n2       1175450108898565888      IQ\n3       1175471073770573824      IQ\n4       1175496913145217024      IQ\n...                     ...     ...\n458192  1019484980282580992      BH\n458193  1021083283709407232      BH\n458194  1017477537889431552      BH\n458195  1022430374696239232      BH\n458196  1022409931029458944      BH\n\n[458197 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>dialect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1175358310087892992</td>\n      <td>IQ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1175416117793349632</td>\n      <td>IQ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1175450108898565888</td>\n      <td>IQ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1175471073770573824</td>\n      <td>IQ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1175496913145217024</td>\n      <td>IQ</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>458192</th>\n      <td>1019484980282580992</td>\n      <td>BH</td>\n    </tr>\n    <tr>\n      <th>458193</th>\n      <td>1021083283709407232</td>\n      <td>BH</td>\n    </tr>\n    <tr>\n      <th>458194</th>\n      <td>1017477537889431552</td>\n      <td>BH</td>\n    </tr>\n    <tr>\n      <th>458195</th>\n      <td>1022430374696239232</td>\n      <td>BH</td>\n    </tr>\n    <tr>\n      <th>458196</th>\n      <td>1022409931029458944</td>\n      <td>BH</td>\n    </tr>\n  </tbody>\n</table>\n<p>458197 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data processing","metadata":{}},{"cell_type":"code","source":"# Marge tweets and dialects datasets in one dataframe\ndata = pd.merge(tweets, dialects, on='id')\n\n# drop the id columns\ndata = data.drop(columns=['id'])\n\ndata","metadata":{"execution":{"iopub.status.busy":"2023-05-22T05:12:22.705015Z","iopub.execute_input":"2023-05-22T05:12:22.709619Z","iopub.status.idle":"2023-05-22T05:12:23.265382Z","shell.execute_reply.started":"2023-05-22T05:12:22.709574Z","shell.execute_reply":"2023-05-22T05:12:23.264460Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                   tweets dialect\n0        @Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .      IQ\n1       @7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...      IQ\n2                         @KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ      IQ\n3              @HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’      IQ\n4                      @hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº      IQ\n...                                                   ...     ...\n458196              @Al_mhbaa_7 Ù…Ø¨Ø³ÙˆØ·ÙŠÙ† Ù…Ù†Ùƒ Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø³Ø·Ø§Ù†Ø§ğŸ˜…      BH\n458197       @Zzainabali @P_ameerah ÙˆØ§Ù„Ù„Ù‡ Ù…Ø§ÙŠÙ†Ø¯Ù‡ Ø§Ø¨Ø´ ÙŠØ®ØªÙŠ      BH\n458198  @Al_mhbaa_7 Ø´Ùˆ Ø¹Ù…Ù„Ù†Ø§ Ù„Ùƒ Ø­Ù†Ø§ ØªÙ‡Ø±Ø¨ÙŠ Ù…Ù†Ù†Ø§ Ø§Ø­Ù†Ø§ Ù…Ø³...      BH\n458199        @haneenalmwla Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙ‡Ø§ ÙˆØ¨Ø§Ù„Ø¹Ø§ÙÙŠÙ‡ ğŸ˜‹ğŸ˜‹ğŸ˜‹      BH\n458200          @jolnar121 Ø§Ù„Ø³Ø­Ù„Ù‡ Ø¶ÙŠÙÙŠ ÙŠ Ø¨ØªØ·Ù„Ø¹ Ù„Ùƒ Ø³Ø­Ù„ÙŠÙ‡ğŸ˜…ğŸ˜…      BH\n\n[458201 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>dialect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .</td>\n      <td>IQ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...</td>\n      <td>IQ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ</td>\n      <td>IQ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’</td>\n      <td>IQ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº</td>\n      <td>IQ</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>458196</th>\n      <td>@Al_mhbaa_7 Ù…Ø¨Ø³ÙˆØ·ÙŠÙ† Ù…Ù†Ùƒ Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø³Ø·Ø§Ù†Ø§ğŸ˜…</td>\n      <td>BH</td>\n    </tr>\n    <tr>\n      <th>458197</th>\n      <td>@Zzainabali @P_ameerah ÙˆØ§Ù„Ù„Ù‡ Ù…Ø§ÙŠÙ†Ø¯Ù‡ Ø§Ø¨Ø´ ÙŠØ®ØªÙŠ</td>\n      <td>BH</td>\n    </tr>\n    <tr>\n      <th>458198</th>\n      <td>@Al_mhbaa_7 Ø´Ùˆ Ø¹Ù…Ù„Ù†Ø§ Ù„Ùƒ Ø­Ù†Ø§ ØªÙ‡Ø±Ø¨ÙŠ Ù…Ù†Ù†Ø§ Ø§Ø­Ù†Ø§ Ù…Ø³...</td>\n      <td>BH</td>\n    </tr>\n    <tr>\n      <th>458199</th>\n      <td>@haneenalmwla Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙ‡Ø§ ÙˆØ¨Ø§Ù„Ø¹Ø§ÙÙŠÙ‡ ğŸ˜‹ğŸ˜‹ğŸ˜‹</td>\n      <td>BH</td>\n    </tr>\n    <tr>\n      <th>458200</th>\n      <td>@jolnar121 Ø§Ù„Ø³Ø­Ù„Ù‡ Ø¶ÙŠÙÙŠ ÙŠ Ø¨ØªØ·Ù„Ø¹ Ù„Ùƒ Ø³Ø­Ù„ÙŠÙ‡ğŸ˜…ğŸ˜…</td>\n      <td>BH</td>\n    </tr>\n  </tbody>\n</table>\n<p>458201 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Dialect names**","metadata":{}},{"cell_type":"code","source":"# Display the dialect names\ndialect_names = data['dialect'].unique()\nprint(dialect_names)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T05:12:23.266993Z","iopub.execute_input":"2023-05-22T05:12:23.268903Z","iopub.status.idle":"2023-05-22T05:12:23.307458Z","shell.execute_reply.started":"2023-05-22T05:12:23.268861Z","shell.execute_reply":"2023-05-22T05:12:23.306304Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['IQ' 'LY' 'QA' 'PL' 'SY' 'TN' 'JO' 'MA' 'SA' 'YE' 'DZ' 'EG' 'LB' 'KW'\n 'OM' 'SD' 'AE' 'BH']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Convert dialectal Arabic names to full country names**","metadata":{}},{"cell_type":"code","source":"# Define a dictionary that maps dialectal Arabic names to full country names\nshort_to_full = {\n    'EG': 'Ù…ØµØ±ÙŠ',\n    'DZ': 'Ø¬Ø²Ø§Ø¦Ø±ÙŠ',\n    'TN': 'ØªÙˆÙ†Ø³ÙŠ',\n    'LY': 'Ù„ÙŠØ¨ÙŠ',\n    'MA': 'Ù…ØºØ±Ø¨ÙŠ',\n    'JO': 'Ø§Ø±Ø¯Ù†ÙŠ',\n    'LB': 'Ù„Ø¨Ù†Ø§Ù†ÙŠ',\n    'PL': 'ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ',\n    'SY': 'Ø³ÙˆØ±ÙŠ',\n    'IQ': 'Ø¹Ø±Ø§Ù‚ÙŠ',\n    'KW': 'ÙƒÙˆÙŠØªÙŠ',\n    'SA': 'Ø³Ø¹ÙˆØ¯ÙŠ',\n    'AE': 'Ø§Ù…Ø§Ø±Ø§ØªÙŠ',\n    'OM': 'Ø¹Ù…Ø§Ù†ÙŠ',\n    'QA': 'Ù‚Ø·Ø±ÙŠ',\n    'YE': 'ÙŠÙ…Ù†ÙŠ',\n    'SD': 'Ø³ÙˆØ¯Ø§Ù†ÙŠ',\n    'BH': 'Ø¨Ø­Ø±ÙŠÙ†ÙŠ'\n}\n# Define a function that converts the short names to full names\ndef convert_name(name):\n    return short_to_full[name]\n\n# Convert dialectal Arabic names to full country names\ndata['dialect'] = data['dialect'].apply(convert_name)\n\ndata","metadata":{"execution":{"iopub.status.busy":"2023-05-22T05:12:23.309276Z","iopub.execute_input":"2023-05-22T05:12:23.309686Z","iopub.status.idle":"2023-05-22T05:12:23.438711Z","shell.execute_reply.started":"2023-05-22T05:12:23.309651Z","shell.execute_reply":"2023-05-22T05:12:23.437570Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                   tweets dialect\n0        @Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .   Ø¹Ø±Ø§Ù‚ÙŠ\n1       @7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...   Ø¹Ø±Ø§Ù‚ÙŠ\n2                         @KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ   Ø¹Ø±Ø§Ù‚ÙŠ\n3              @HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’   Ø¹Ø±Ø§Ù‚ÙŠ\n4                      @hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº   Ø¹Ø±Ø§Ù‚ÙŠ\n...                                                   ...     ...\n458196              @Al_mhbaa_7 Ù…Ø¨Ø³ÙˆØ·ÙŠÙ† Ù…Ù†Ùƒ Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø³Ø·Ø§Ù†Ø§ğŸ˜…  Ø¨Ø­Ø±ÙŠÙ†ÙŠ\n458197       @Zzainabali @P_ameerah ÙˆØ§Ù„Ù„Ù‡ Ù…Ø§ÙŠÙ†Ø¯Ù‡ Ø§Ø¨Ø´ ÙŠØ®ØªÙŠ  Ø¨Ø­Ø±ÙŠÙ†ÙŠ\n458198  @Al_mhbaa_7 Ø´Ùˆ Ø¹Ù…Ù„Ù†Ø§ Ù„Ùƒ Ø­Ù†Ø§ ØªÙ‡Ø±Ø¨ÙŠ Ù…Ù†Ù†Ø§ Ø§Ø­Ù†Ø§ Ù…Ø³...  Ø¨Ø­Ø±ÙŠÙ†ÙŠ\n458199        @haneenalmwla Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙ‡Ø§ ÙˆØ¨Ø§Ù„Ø¹Ø§ÙÙŠÙ‡ ğŸ˜‹ğŸ˜‹ğŸ˜‹  Ø¨Ø­Ø±ÙŠÙ†ÙŠ\n458200          @jolnar121 Ø§Ù„Ø³Ø­Ù„Ù‡ Ø¶ÙŠÙÙŠ ÙŠ Ø¨ØªØ·Ù„Ø¹ Ù„Ùƒ Ø³Ø­Ù„ÙŠÙ‡ğŸ˜…ğŸ˜…  Ø¨Ø­Ø±ÙŠÙ†ÙŠ\n\n[458201 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>dialect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .</td>\n      <td>Ø¹Ø±Ø§Ù‚ÙŠ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...</td>\n      <td>Ø¹Ø±Ø§Ù‚ÙŠ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ</td>\n      <td>Ø¹Ø±Ø§Ù‚ÙŠ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’</td>\n      <td>Ø¹Ø±Ø§Ù‚ÙŠ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº</td>\n      <td>Ø¹Ø±Ø§Ù‚ÙŠ</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>458196</th>\n      <td>@Al_mhbaa_7 Ù…Ø¨Ø³ÙˆØ·ÙŠÙ† Ù…Ù†Ùƒ Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø³Ø·Ø§Ù†Ø§ğŸ˜…</td>\n      <td>Ø¨Ø­Ø±ÙŠÙ†ÙŠ</td>\n    </tr>\n    <tr>\n      <th>458197</th>\n      <td>@Zzainabali @P_ameerah ÙˆØ§Ù„Ù„Ù‡ Ù…Ø§ÙŠÙ†Ø¯Ù‡ Ø§Ø¨Ø´ ÙŠØ®ØªÙŠ</td>\n      <td>Ø¨Ø­Ø±ÙŠÙ†ÙŠ</td>\n    </tr>\n    <tr>\n      <th>458198</th>\n      <td>@Al_mhbaa_7 Ø´Ùˆ Ø¹Ù…Ù„Ù†Ø§ Ù„Ùƒ Ø­Ù†Ø§ ØªÙ‡Ø±Ø¨ÙŠ Ù…Ù†Ù†Ø§ Ø§Ø­Ù†Ø§ Ù…Ø³...</td>\n      <td>Ø¨Ø­Ø±ÙŠÙ†ÙŠ</td>\n    </tr>\n    <tr>\n      <th>458199</th>\n      <td>@haneenalmwla Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙ‡Ø§ ÙˆØ¨Ø§Ù„Ø¹Ø§ÙÙŠÙ‡ ğŸ˜‹ğŸ˜‹ğŸ˜‹</td>\n      <td>Ø¨Ø­Ø±ÙŠÙ†ÙŠ</td>\n    </tr>\n    <tr>\n      <th>458200</th>\n      <td>@jolnar121 Ø§Ù„Ø³Ø­Ù„Ù‡ Ø¶ÙŠÙÙŠ ÙŠ Ø¨ØªØ·Ù„Ø¹ Ù„Ùƒ Ø³Ø­Ù„ÙŠÙ‡ğŸ˜…ğŸ˜…</td>\n      <td>Ø¨Ø­Ø±ÙŠÙ†ÙŠ</td>\n    </tr>\n  </tbody>\n</table>\n<p>458201 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# preprocess Text\n\n**Filter data from some Unwanted additions, such as :**\n> symbols (such as @ # .. etc)\n\n> stopwords ( such as ÙÙŠ , Ø§Ù„ Ø§Ù„ØªØ¹Ø±ÙŠÙ )\n\n> Reducing a word to its stem using stemming","metadata":{}},{"cell_type":"code","source":"def preprocessText(text):\n    # Remove URLs, mentions, and hashtags\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    text = re.sub(r'@\\w+|\\#\\w+', '', text)\n\n    # Tokenize the text\n    tokens = word_tokenize(text)\n\n    # Remove stop words\n    stop_words = set(stopwords.words('arabic'))\n    tokens = [word for word in tokens if word not in stop_words]\n\n    # Perform stemming\n    stemmer = ISRIStemmer()\n    tokens = [stemmer.stem(word) for word in tokens]\n\n    # Join the tokens back into a string\n    preprocessed_text = ' '.join(tokens)\n\n    return preprocessed_text","metadata":{"execution":{"iopub.status.busy":"2023-05-22T05:12:23.440004Z","iopub.execute_input":"2023-05-22T05:12:23.441041Z","iopub.status.idle":"2023-05-22T05:12:23.449097Z","shell.execute_reply.started":"2023-05-22T05:12:23.441004Z","shell.execute_reply":"2023-05-22T05:12:23.447800Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# splt the data","metadata":{}},{"cell_type":"code","source":"# list of column names\ncolumn_names = ['tweets','dialect'] \n\n# Train data\ntrain_data = data.sample(frac = 0.75) # Take 75% of the data randomly\ntrain_data.columns = column_names # Label the columns\nx_train = train_data.tweets # x = the tweets in train data\ny_train = train_data.dialect # y = the labels of the train data\n\n# Test data\ntest_data = data.drop(train_data.index) # Take the remaining 25% of the data\ntest_data.columns = column_names # Label the columns\nx_test = test_data.tweets # x = the tweets in test data\ny_test = test_data.dialect # y = the labels of the test data","metadata":{"execution":{"iopub.status.busy":"2023-05-22T05:12:23.451110Z","iopub.execute_input":"2023-05-22T05:12:23.451470Z","iopub.status.idle":"2023-05-22T05:12:23.585246Z","shell.execute_reply.started":"2023-05-22T05:12:23.451435Z","shell.execute_reply":"2023-05-22T05:12:23.584296Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Traning","metadata":{}},{"cell_type":"code","source":"# Filter the train data\nx_train = x_train.apply(lambda x: preprocessText(x))\n  \n# Feature extraction\n# Define a custom analyzer that applies stemming to the tokens\nstemmer = ISRIStemmer()\nanalyzer = TfidfVectorizer().build_analyzer()\ndef stemmed_words(doc):\n    return (stemmer.stem(w) for w in analyzer(doc))\n \n# Create a Vectorizer Object\nvectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000, analyzer=stemmed_words) \n \n# Fit and transform the Vectorizer Object on the train data\nx_train = vectorizer.fit_transform(x_train) \ny_train = y_train.values # The true labels of the train data\n\n# Define the SVM model\nmodel = LinearSVC()\n# Define the hyperparameters to tune\nhyperparameters = { \n    'C': [0.1, 1, 10],\n    'penalty': ['l1', 'l2'] \n}\n\n# Use GridSearchCV to find the best hyperparameters\ngrid = GridSearchCV(model, hyperparameters)\ngrid.fit(x_train, y_train)\n\n# Print the best hyperparameters\nprint(\"Best hyperparameters: \", grid.best_params_)\n\n# Get the best model\nbest_model = grid.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2023-05-22T05:12:23.588755Z","iopub.execute_input":"2023-05-22T05:12:23.589133Z","iopub.status.idle":"2023-05-22T05:43:13.525175Z","shell.execute_reply.started":"2023-05-22T05:12:23.589094Z","shell.execute_reply":"2023-05-22T05:43:13.524059Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:544: UserWarning: The parameter 'ngram_range' will not be used since 'analyzer' is callable'\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n15 fits failed out of a total of 30.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n15 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n    raise ValueError(\nValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.42912432        nan 0.43136496        nan 0.43002058]\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Best hyperparameters:  {'C': 1, 'penalty': 'l2'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\n\n# Preprocess the test data\nx_test = x_test.apply(preprocessText)\n\n# Feature extraction\nx_test = vectorizer.transform(x_test) # Encode the data\n\n# Predict the labels of the test data using the best model\ny_test = y_test.values # The true labels of the test data\ny_predict = grid.predict(x_test)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, y_predict)\nprint(\"Accuracy: \", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T05:43:13.527002Z","iopub.execute_input":"2023-05-22T05:43:13.527694Z","iopub.status.idle":"2023-05-22T05:44:46.615772Z","shell.execute_reply.started":"2023-05-22T05:43:13.527655Z","shell.execute_reply":"2023-05-22T05:44:46.614767Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Accuracy:  0.4327106067219555\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Arabic Dialect Identification system","metadata":{}},{"cell_type":"code","source":"# Take text from user\nuserText = input(\"ÙØ¶Ù„Ø§Ù‹ Ø§Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ:\")\n\n# Convert string into an DataFrame\ntext = [userText]\ntext = pd.DataFrame(text)\n\n# Filter the text\ntext[0] = text[0].apply(preprocessText)\n\n# Encode the text\ntext_user = vectorizer.transform(text[0]) \n\n# The prediction made by the model\npredict = best_model.predict(text_user)\n\n# Display the result\nprint(\"Ù„Ù‡Ø¬Ù‡ Ø§Ù„Ù†Øµ Ù‡ÙŠ:\",predict[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-22T08:37:50.538098Z","iopub.execute_input":"2023-05-22T08:37:50.538480Z","iopub.status.idle":"2023-05-22T08:38:01.316332Z","shell.execute_reply.started":"2023-05-22T08:37:50.538449Z","shell.execute_reply":"2023-05-22T08:38:01.315381Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdin","text":"ÙØ¶Ù„Ø§Ù‹ Ø§Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ: Ø§Ø²ÙŠÙƒ ÙŠØ§ Ø¨Ø§Ø´Ø§\n"},{"name":"stdout","text":"Ù„Ù‡Ø¬Ù‡ Ø§Ù„Ù†Øµ Ù‡ÙŠ: Ù…ØµØ±ÙŠ\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n\n## Contributors\n> Section: AIL\n\n> Raneem Saad Alomari, ID: 2006352\n\n> Bedoor Ayad Alsulami, ID: 2005961\n\n> Layal Soud Halwani, ID: 2007896\n\n> Afnan Tariq Algogandi, ID: 2007926 \n","metadata":{}}]}